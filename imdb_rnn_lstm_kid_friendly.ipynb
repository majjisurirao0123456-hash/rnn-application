{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1012d12b",
   "metadata": {},
   "source": [
    "# RNN / LSTM for Kids: Movie Review Feelings (IMDB Sentiment)\n",
    "\n",
    "This notebook teaches how an **RNN** (Recurrent Neural Network) learns from **sequences** (ordered things),\n",
    "like words in a sentence.\n",
    "\n",
    "We will use the built-in Keras dataset **IMDB**:\n",
    "- movie reviews (text)\n",
    "- labels: **0 = negative** ðŸ˜ž, **1 = positive** ðŸ˜„\n",
    "\n",
    "Even though itâ€™s kid-friendly, it uses real deep learning ideas:\n",
    "- **tokenization** (words â†’ numbers)\n",
    "- **padding** (make sequences the same length)\n",
    "- **Embedding** (numbers â†’ meaning vectors)\n",
    "- **RNN / LSTM** (reads words in order)\n",
    "- **Dropout** (prevents memorizing)\n",
    "- training **history**, graphs, evaluation\n",
    "- choose a test review and **predict** its sentiment\n",
    "\n",
    "---\n",
    "\n",
    "## Big intuition\n",
    "\n",
    "A CNN is great for images (space patterns).\n",
    "\n",
    "An RNN is great for sequences (time/order patterns).\n",
    "\n",
    "Think of an RNN like a kid reading a sentence **word by word**:\n",
    "- It keeps a small **memory** of what it read so far.\n",
    "- That memory changes as each new word arrives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97c213",
   "metadata": {},
   "source": [
    "## 0) Install / Imports\n",
    "\n",
    "If you are using Colab, TensorFlow is usually installed.\n",
    "\n",
    "If running locally (terminal):\n",
    "\n",
    "```bash\n",
    "pip install tensorflow matplotlib numpy ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f2b88",
   "metadata": {},
   "source": [
    "## 1) Load the IMDB dataset (built-in)\n",
    "\n",
    "Keras gives IMDB reviews already converted into **integers** (word IDs).\n",
    "That is perfect for neural networks.\n",
    "\n",
    "We choose:\n",
    "- `num_words = 10000` â†’ keep only the 10,000 most common words.\n",
    "\n",
    "Each review becomes something like:\n",
    "`[1, 14, 20, 2, 56, ...]` (a list of integers)\n",
    "\n",
    "Label:\n",
    "- 0 = negative\n",
    "- 1 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a968621",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000  # vocabulary size (how many different word IDs we keep)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=num_words)\n",
    "\n",
    "print(\"Number of training reviews:\", len(x_train))\n",
    "print(\"Number of test reviews    :\", len(x_test))\n",
    "\n",
    "print(\"Example review (as word IDs):\", x_train[0][:30], \"...\")\n",
    "print(\"Example label:\", y_train[0])\n",
    "\n",
    "# Reviews are variable length (not all same size)\n",
    "lengths = [len(r) for r in x_train]\n",
    "print(\"Min length:\", min(lengths), \"Max length:\", max(lengths), \"Average length:\", sum(lengths)/len(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d946e",
   "metadata": {},
   "source": [
    "### What is `shape` for sequences?\n",
    "\n",
    "A single review is not a rectangle like an image.\n",
    "It is a **list** of word IDs, and different reviews have different lengths.\n",
    "\n",
    "But neural networks love rectangles (same length).\n",
    "\n",
    "So we use **padding** to make every review the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ef9c8",
   "metadata": {},
   "source": [
    "## 2) Padding: make all reviews the same length\n",
    "\n",
    "We pick a maximum length `max_len`.\n",
    "\n",
    "- If a review is shorter â†’ add zeros at the front (or end)\n",
    "- If a review is longer â†’ cut it\n",
    "\n",
    "`0` will mean: â€œempty paddingâ€.\n",
    "\n",
    "After padding, `x_train_padded` becomes a big rectangle:\n",
    "`(num_reviews, max_len)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700abfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200  # keep last 200 words (good for a demo)\n",
    "\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train, maxlen=max_len, padding=\"pre\", truncating=\"pre\"\n",
    ")\n",
    "x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test, maxlen=max_len, padding=\"pre\", truncating=\"pre\"\n",
    ")\n",
    "\n",
    "print(\"x_train_padded shape:\", x_train_padded.shape)  # (25000, 200)\n",
    "print(\"x_test_padded shape :\", x_test_padded.shape)   # (25000, 200)\n",
    "\n",
    "print(\"First padded review (first 30 IDs):\", x_train_padded[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5609631",
   "metadata": {},
   "source": [
    "## 3) Embedding: turning word IDs into meaning vectors\n",
    "\n",
    "Right now each word is just an integer ID:\n",
    "- `42` doesn't \"mean\" anything by itself.\n",
    "\n",
    "**Embedding** is like giving each word a little â€œmeaning arrowâ€ in space:\n",
    "- Each word becomes a vector like `[0.1, -0.3, 0.7, ...]`\n",
    "\n",
    "Embedding layer shape idea:\n",
    "- Input: `(batch_size, max_len)`  â†’ integers\n",
    "- Output: `(batch_size, max_len, embed_dim)` â†’ vectors\n",
    "\n",
    "Example:\n",
    "- `max_len = 200`\n",
    "- `embed_dim = 32`\n",
    "\n",
    "Then each review becomes a **row of 200 word-vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cdaf9",
   "metadata": {},
   "source": [
    "## 4) RNN / LSTM: reading in order\n",
    "\n",
    "An RNN reads the sequence step-by-step.\n",
    "\n",
    "Think of a little robot reading words:\n",
    "- It sees one word vector\n",
    "- Updates its memory\n",
    "- Moves to the next word\n",
    "\n",
    "A basic RNN can forget long-distance info.\n",
    "So we often use **LSTM** (Long Short-Term Memory),\n",
    "which has a smarter memory system.\n",
    "\n",
    "We'll use **LSTM** because it works well and is common.\n",
    "\n",
    "### Output choices\n",
    "- `return_sequences=False` (default): LSTM returns only the final memory state.\n",
    "That final state is like: â€œMy final understanding of the whole review.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd8988",
   "metadata": {},
   "source": [
    "## 5) Build the model (Keras)\n",
    "\n",
    "We will build:\n",
    "\n",
    "1. **Embedding** (word IDs â†’ vectors)\n",
    "2. **LSTM** (reads 200 steps)\n",
    "3. **Dropout** (anti-memorizing)\n",
    "4. **Dense(1)** with sigmoid (probability of positive)\n",
    "\n",
    "Why sigmoid?\n",
    "- It outputs a number between 0 and 1\n",
    "- close to 1 â†’ positive\n",
    "- close to 0 â†’ negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(max_len,)),\n",
    "\n",
    "    # Turn word IDs into vectors\n",
    "    tf.keras.layers.Embedding(input_dim=num_words, output_dim=embed_dim),\n",
    "\n",
    "    # LSTM reads the sequence\n",
    "    tf.keras.layers.LSTM(64),\n",
    "\n",
    "    # Dropout to reduce overfitting\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    # One output: probability of positive review\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0def51",
   "metadata": {},
   "source": [
    "## 6) Train the model\n",
    "\n",
    "We train for a few epochs (demo-friendly).\n",
    "\n",
    "We also use `validation_split=0.2`:\n",
    "- 80% training\n",
    "- 20% validation (mini test while training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ab4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train_padded, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979aa5fc",
   "metadata": {},
   "source": [
    "## 7) Plot training history\n",
    "\n",
    "- Loss should go down\n",
    "- Accuracy should go up\n",
    "\n",
    "If validation accuracy goes down while training accuracy goes up,\n",
    "the model might be memorizing (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history.history\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist[\"loss\"], label=\"train loss\")\n",
    "plt.plot(hist[\"val_loss\"], label=\"val loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist[\"accuracy\"], label=\"train acc\")\n",
    "plt.plot(hist[\"val_accuracy\"], label=\"val acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cf280",
   "metadata": {},
   "source": [
    "## 8) Evaluate on test set\n",
    "\n",
    "Now we check accuracy on reviews the model never trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test_padded, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", float(test_acc))\n",
    "print(\"Test loss    :\", float(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7f7ea",
   "metadata": {},
   "source": [
    "## 9) Decode reviews back into words (so humans can read them)\n",
    "\n",
    "IMDB dataset has a word index dictionary.\n",
    "\n",
    "We will:\n",
    "- get `word_index` mapping: word â†’ id\n",
    "- invert it to: id â†’ word\n",
    "- decode a review (list of IDs) into text\n",
    "\n",
    "Important detail:\n",
    "Keras IMDB uses special reserved IDs:\n",
    "- 0: padding\n",
    "- 1: start of sequence\n",
    "- 2: unknown word\n",
    "- 3: unused\n",
    "\n",
    "So when decoding we often subtract 3 from IDs, or use an offset.\n",
    "Keras uses an offset of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32264e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mapping word -> id\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# Invert to id -> word\n",
    "id_to_word = {idx + 3: word for word, idx in word_index.items()}\n",
    "id_to_word[0] = \"<PAD>\"\n",
    "id_to_word[1] = \"<START>\"\n",
    "id_to_word[2] = \"<UNK>\"\n",
    "id_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "def decode_review(review_ids):\n",
    "    # Convert list of ints to a readable sentence\n",
    "    words = [id_to_word.get(i, \"<UNK>\") for i in review_ids]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Show one decoded training review\n",
    "example_idx = 0\n",
    "print(\"Label:\", y_train[example_idx], \"(1=positive, 0=negative)\")\n",
    "print(decode_review(x_train[example_idx][:60]), \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef141d06",
   "metadata": {},
   "source": [
    "## 10) Predict one test review\n",
    "\n",
    "We will:\n",
    "- take a test review\n",
    "- pad it\n",
    "- model outputs a probability `p`\n",
    "- if `p >= 0.5` â†’ positive else negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d363cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "review_ids = x_test[idx]\n",
    "true_label = int(y_test[idx])\n",
    "\n",
    "review_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    [review_ids], maxlen=max_len, padding=\"pre\", truncating=\"pre\"\n",
    ")\n",
    "\n",
    "p = float(model.predict(review_padded, verbose=0)[0][0])\n",
    "pred_label = 1 if p >= 0.5 else 0\n",
    "\n",
    "print(\"True label:\", true_label, \"->\", \"positive\" if true_label==1 else \"negative\")\n",
    "print(\"Pred prob positive:\", round(p, 4))\n",
    "print(\"Pred label:\", pred_label, \"->\", \"positive\" if pred_label==1 else \"negative\")\n",
    "\n",
    "print(\"\\nReview snippet (decoded):\")\n",
    "print(decode_review(review_ids[:80]), \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035ab86",
   "metadata": {},
   "source": [
    "## 11) Choose another test review and predict (interactive)\n",
    "\n",
    "If `ipywidgets` works:\n",
    "- use a slider to pick any test index\n",
    "- see decoded text + prediction\n",
    "\n",
    "If widgets do not work:\n",
    "- change `idx = ...` manually and run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(idx: int, words_to_show: int = 120):\n",
    "    review_ids = x_test[idx]\n",
    "    true_label = int(y_test[idx])\n",
    "\n",
    "    review_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [review_ids], maxlen=max_len, padding=\"pre\", truncating=\"pre\"\n",
    "    )\n",
    "\n",
    "    p = float(model.predict(review_padded, verbose=0)[0][0])\n",
    "    pred_label = 1 if p >= 0.5 else 0\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test index: {idx}\")\n",
    "    print(\"True:\", \"positive\" if true_label==1 else \"negative\")\n",
    "    print(\"Pred prob positive:\", round(p, 4))\n",
    "    print(\"Pred:\", \"positive\" if pred_label==1 else \"negative\")\n",
    "    print(\"-\"*80)\n",
    "    print(decode_review(review_ids[:words_to_show]))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    slider = widgets.IntSlider(value=0, min=0, max=len(x_test)-1, step=1, description=\"Test idx:\")\n",
    "    words = widgets.IntSlider(value=120, min=30, max=300, step=10, description=\"Words:\")\n",
    "    ui = widgets.interactive_output(show_prediction, {\"idx\": slider, \"words_to_show\": words})\n",
    "    display(slider, words, ui)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available here. Manual mode works!\")\n",
    "    idx = 123\n",
    "    show_prediction(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d87943",
   "metadata": {},
   "source": [
    "## 12) (Optional) Swap LSTM for SimpleRNN or GRU\n",
    "\n",
    "Try changing the model:\n",
    "\n",
    "- `tf.keras.layers.SimpleRNN(64)` (simpler, can forget more)\n",
    "- `tf.keras.layers.GRU(64)` (like LSTM, often faster)\n",
    "\n",
    "The rest stays the same.\n",
    "\n",
    "---\n",
    "\n",
    "# Quick quiz (check understanding)\n",
    "\n",
    "1. Why do we need **padding** for reviews?\n",
    "2. What is the difference between a **word ID** and an **embedding vector**?\n",
    "3. What does an **LSTM** do that helps with sequences?\n",
    "4. Why do we use **sigmoid** at the end instead of softmax?\n",
    "5. If the model outputs `p = 0.92`, what does that mean?\n",
    "6. If `max_len = 200`, what is the shape of `x_train_padded`?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
